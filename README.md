
# ğŸš€ GPS Data Processing Pipeline com Apache Spark

Este projeto demonstra uma pipeline simples de engenharia de dados utilizando **Apache Spark** para processar dados simulados de GPS. Os dados sÃ£o gerados com Python (Pandas) e processados com Spark para anÃ¡lises bÃ¡sicas.

---

## ğŸ›  Tecnologias Utilizadas

- **Python 3**
- **Pandas**
- **Apache Spark 3**
- **Docker + Docker Compose**
- **Bitnami Spark Image**
- **TQDM (barra de progresso)**
- **PySpark**

---

## ğŸ“¦ Funcionalidades

- GeraÃ§Ã£o de dados sintÃ©ticos de localizaÃ§Ã£o GPS com `generate_csv.py`:
  - 100 veÃ­culos simulados
  - 100.000 registros por veÃ­culo
  - Timestamps sequenciais com 1s de intervalo
- Processamento com Spark no `spark_pipeline.py`:
  - Filtro de dados por veÃ­culo
  - CÃ¡lculo de velocidade mÃ©dia por veÃ­culo
  - Salvamento em CSV dos resultados processados

---

## ğŸ“‚ Estrutura do Projeto

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ entrypoint.sh
â”œâ”€â”€ generate_csv.py
â”œâ”€â”€ spark_pipeline.py
â”œâ”€â”€ csv_data/
â”‚   â””â”€â”€ gps_data.csv (gerado automaticamente)
â””â”€â”€ output/
    â””â”€â”€ avg_speed/ (resultados em CSV)
```

---

## ğŸ§° PrÃ©-requisitos

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)

---

## ğŸš€ Como rodar o projeto

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/seu-repo.git
cd seu-repo
```

### 2. Construa a imagem Docker

```bash
docker compose build
```

### 3. Suba o container

```bash
docker compose up
```

Esse comando irÃ¡:

- Criar a pasta `/opt/bitnami/sparkuser` para armazenar arquivos temporÃ¡rios usados pelo Spark (configuraÃ§Ã£o interna da imagem).
- Gerar o arquivo `csv_data/gps_data.csv` com 10 milhÃµes de linhas (se ainda nÃ£o existir).
- Processar esse CSV com Apache Spark.
- Mostrar no terminal:
  - Os dados do veÃ­culo `V1`
  - A mÃ©dia de velocidade de todos os veÃ­culos
- Salvar o resultado no diretÃ³rio `output/avg_speed/`.

---

## ğŸ§  Expertise demonstrada

- CriaÃ§Ã£o de dados sintÃ©ticos de larga escala com Python e Pandas
- Uso de PySpark para ETL (Extract, Transform, Load)
- ManipulaÃ§Ã£o de grandes volumes de dados com Spark
- Empacotamento completo do projeto com Docker, respeitando permissÃµes e seguranÃ§a
- OrganizaÃ§Ã£o modular de scripts e boas prÃ¡ticas de cÃ³digo
- Conhecimento de containers, volumes e automaÃ§Ã£o com `entrypoint.sh`

---

## ğŸ“ ObservaÃ§Ãµes importantes

- A imagem Bitnami Spark usa um usuÃ¡rio nÃ£o-root com UID 1001 para rodar o Spark, por isso configuramos permissÃµes e diretÃ³rios no Dockerfile para evitar erros de permissÃ£o.
- A variÃ¡vel de ambiente `HOME` foi redefinida para um caminho dentro de `/opt/bitnami` onde o usuÃ¡rio 1001 tem permissÃ£o de escrita. Isso evita erros comuns relacionados a diretÃ³rios home inexistentes ou sem permissÃ£o.
- O script `entrypoint.sh` automatiza o fluxo dentro do container, gerando o CSV e processando com Spark em sequÃªncia.
- O diretÃ³rio `csv_data/` e `output/avg_speed/` sÃ£o criados automaticamente e usados para entrada e saÃ­da de dados.

---

## ğŸ“¬ Contato

Em caso de dÃºvidas ou sugestÃµes, sinta-se Ã  vontade para abrir uma [issue](https://github.com/seu-usuario/seu-repo/issues) ou enviar um pull request.
